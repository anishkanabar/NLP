{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6cVuYKLo-sz"
      },
      "source": [
        "# Neural Network-based Part of Speech Tagger\n",
        "\n",
        "In this problem, we will\n",
        "- Build a neural network based part of speech (POS) tagger.\n",
        "- Run the model on a GPU at Google Colab.\n",
        "- Extend the model to use pretrained embeddings.\n",
        "- Optionally extend the model in some manner of your own choosing.\n",
        "\n",
        "There are a total of 8 tasks.\n",
        "\n",
        "PyTorch and, particularly, Torchtext are currently evolving rapidly. The code in this homework works for PyTorch version 1.11.0 and Torchtext version 0.12.0. It hasn't been tested for other versions.\n",
        "\n",
        "Acknowledgements: Part of this homework were adapted from work done by\n",
        "[Ben Trevett](https://github.com/bentrevett) and\n",
        "[Kevin Gimpel](https://home.ttic.edu/~kgimpel/)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "tHE8h1A0CHAV",
        "outputId": "18c474dc-8467-41af-b506-ce31e27c6819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.4.0-cp37-cp37m-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 26.1 MB/s \n",
            "\u001b[?25hCollecting torch==1.12.0\n",
            "  Downloading torch-1.12.0-cp37-cp37m-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.3 MB 19 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.10-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 69.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.0->torchdata) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.6.15)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 35.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Installing collected packages: urllib3, torch, portalocker, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0\n",
            "    Uninstalling torch-1.9.0:\n",
            "      Successfully uninstalled torch-1.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.12.0 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.12.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.12.0 which is incompatible.\n",
            "fastai 2.6.3 requires torch<1.12,>=1.7.0, but you have torch 1.12.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed portalocker-2.4.0 torch-1.12.0 torchdata-0.4.0 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "b739ZVJHp_VN",
        "outputId": "124e0e8a-4023-42f1-abca-44f052c030ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.10.0 in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Collecting torch==1.9.0\n",
            "  Using cached torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.25.11)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0\n",
            "    Uninstalling torch-1.12.0:\n",
            "      Successfully uninstalled torch-1.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\n",
            "torchdata 0.4.0 requires torch==1.12.0, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U torchtext==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArRy2xKUo-s4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "#import torchdata\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab, vocab\n",
        "\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-hiSlAto-s6"
      },
      "outputs": [],
      "source": [
        "SEED = 53113 # Specify a seed for reproducibility\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c_VNhDTo-s7"
      },
      "source": [
        "### Using GPUs\n",
        "\n",
        "We want our code to run on a CPU or on a [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit) when available, as on Google Colab.  The code will likely run at least five times as fast on a GPU. PyTorch connects with a GPU via the system interface [CUDA](https://en.wikipedia.org/wiki/CUDA).\n",
        "\n",
        "The following tells us whether CUDA/GPU is available or not.  The changes you have to make so that your code can use a GPU when available are surprisingly small. All such changes have already been made in the provided code below. They are:\n",
        "- the data loader needs to place the input to the model appropriately using `to(device).`\n",
        "- similarly, the model object itself needs to be placed appropriately using `to(device).`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Peazo6zfo-s8"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqvY_KN6o-s8"
      },
      "source": [
        "### Cache for GloVe\n",
        "\n",
        "If you have already downloaded GloVe (in this notebook or in the logistic regression notebook), you can reuse it. Change the vector cache directory location if it is saved in a different location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjiD_QPLo-s9"
      },
      "outputs": [],
      "source": [
        "VECTORS_CACHE_DIR = './.vector_cache' # or modify to the correct location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqhhSXrfo-s-"
      },
      "source": [
        "### UDPOS Dataset\n",
        "\n",
        "We'll work with the [`UDPOS`](https://pytorch.org/text/stable/datasets.html#udpos) dataset included with torchtext.  It contains about 12,500 sentences and the POS tags for the each word.  There are, in fact, two sets of tags, based on two different POS tagging standards: [Universal Dependency](https://universaldependencies.org/u/pos/) (UD) and [Penn Treebank](https://www.sketchengine.eu/penn-treebank-tagset/) (PTB). It has additionally about 2000 sentences in the validation and test sets.  The words and corresonding tags are organized into lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT29odrYo-s_",
        "outputId": "46b49d7e-3908-42d8-e4c2-48ff68f7f1fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "en-ud-v2.zip: 100%|██████████| 688k/688k [00:00<00:00, 31.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence for Example 0 ---\n",
            "['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.']\n",
            "The UD tags for Example 0 ---\n",
            "['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n",
            "The PTB tags for Example 0 ---\n",
            "['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']\n",
            "\n",
            "The sentence for Example 1 ---\n",
            "['[', 'This', 'killing', 'of', 'a', 'respected', 'cleric', 'will', 'be', 'causing', 'us', 'trouble', 'for', 'years', 'to', 'come', '.', ']']\n",
            "The UD tags for Example 1 ---\n",
            "['PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'AUX', 'AUX', 'VERB', 'PRON', 'NOUN', 'ADP', 'NOUN', 'PART', 'VERB', 'PUNCT', 'PUNCT']\n",
            "The PTB tags for Example 1 ---\n",
            "['-LRB-', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'MD', 'VB', 'VBG', 'PRP', 'NN', 'IN', 'NNS', 'TO', 'VB', '.', '-RRB-']\n",
            "\n",
            "The sentence for Example 2 ---\n",
            "['DPA', ':', 'Iraqi', 'authorities', 'announced', 'that', 'they', 'had', 'busted', 'up', '3', 'terrorist', 'cells', 'operating', 'in', 'Baghdad', '.']\n",
            "The UD tags for Example 2 ---\n",
            "['PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'SCONJ', 'PRON', 'AUX', 'VERB', 'ADP', 'NUM', 'ADJ', 'NOUN', 'VERB', 'ADP', 'PROPN', 'PUNCT']\n",
            "The PTB tags for Example 2 ---\n",
            "['NNP', ':', 'JJ', 'NNS', 'VBD', 'IN', 'PRP', 'VBD', 'VBN', 'RP', 'CD', 'JJ', 'NNS', 'VBG', 'IN', 'NNP', '.']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_iter = datasets.UDPOS(split='train')\n",
        "\n",
        "for i, example in enumerate(train_iter):\n",
        "    print(f'The sentence for Example {i} ---')\n",
        "    print(example[0])\n",
        "    print(f'The UD tags for Example {i} ---')\n",
        "    print(example[1])\n",
        "    print(f'The PTB tags for Example {i} ---')\n",
        "    print(example[2])\n",
        "    print()\n",
        "    if i == 2: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2E7SKpRo-s_"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "In our neural network-based model we can only accept fixed sized inputs.  Each input corresponds to a subsequence, consisting of a  center word and a number $w$ of words before and after, and the label is the POS tag for the center word. We often refer to the subsequence as a window of size $(2w + 1)$.\n",
        "\n",
        "So given a sentence, say, \"This killing of a respected cleric.' with, tags, say 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT' (the last tag is for the period at the end of the sentence), some of our input examples for the model are the following (for $w = 1$):\n",
        "- \"This killing of\", 'NOUN'\n",
        "- \"killing of a\", 'ADP'\n",
        "- \"respected cleric.\", 'NOUN'.\n",
        "\n",
        "But the above scheme would imply that, when $w = 1$, the tags of the first and last words for any sentence would never be part of an example.  To avoid that, we add dummy '\\<s>' and '\\</s>' words, and corresponding tags, to the beginning and end of each sentence.  We need to add as many dummy words as $w$.  So if $w = 2$, we first convert the above sentence and its tags to\n",
        "- \"\\<s> \\<s> This killing of a respected cleric. \\</s> \\</s>' \n",
        "- 'STAG', 'STAG','DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'ETAG', 'ETAG'. \n",
        "\n",
        "**Task 1** [5 points]: Most of the code for preprocessing is given below.  But some parts are missing.  Complete the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlmXQNQQo-tA",
        "outputId": "8fe25001-77e5-4672-c05e-59edadaca3e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed.\n"
          ]
        }
      ],
      "source": [
        "W = 1\n",
        "WINDOW_SIZE = (2 * W + 1)\n",
        "\n",
        "SENT_START_WORD = '<s>'\n",
        "SENT_END_WORD = '</s>'\n",
        "SENT_START_TAG = '<STAG>'\n",
        "SENT_END_TAG = '<ETAG>'\n",
        "\n",
        "def add_sent_start_end(data_iter, w):\n",
        "    for (words, ud_tags, ptb_tags) in data_iter:\n",
        "        new_words = [SENT_START_WORD] * w + words + [SENT_END_WORD] * w\n",
        "        new_ud_tags = [SENT_START_TAG] * w+ ud_tags + [SENT_END_TAG] * w\n",
        "        ## MISSING PART: ADD YOUR CODE BELOW\n",
        "        new_ptb_tags = [SENT_START_TAG] * w+ ptb_tags + [SENT_END_TAG] * w\n",
        "        ## ADD YOUR CODE ABOVE\n",
        "        yield(new_words, new_ud_tags, new_ptb_tags)\n",
        "        \n",
        "def create_windows(data_iter, w):\n",
        "    window_size = 2*w + 1\n",
        "    for (words, ud_tags, ptb_tags) in data_iter:\n",
        "        words_zip = zip(*[words[i:] for i in range(window_size)])\n",
        "        ud_zip = zip(*[ud_tags[i:] for i in range(window_size)])\n",
        "        ## MISSING PART: ADD YOUR CODE BELOW\n",
        "        ptb_zip = zip(*[ptb_tags[i:] for i in range(window_size)])\n",
        "        ## ADD YOUR CODE ABOVE\n",
        "        for word_sseq, ud_sseq, ptb_sseq in zip(\n",
        "                words_zip, ud_zip, ptb_zip):\n",
        "            yield(word_sseq, ud_sseq, ptb_sseq)\n",
        "            \n",
        "def preprocess_data_seq(data_iter, w):\n",
        "    ## MISSING PART: ADD YOUR CODE BELOW\n",
        "    data_iter_a = add_sent_start_end(data_iter, w)\n",
        "    data_iter_b = create_windows(data_iter_a, w)\n",
        "    return data_iter_b\n",
        "\n",
        "def test_preprocess_data_seq():\n",
        "    \n",
        "    # WARNING: The following test assumes a particular default\n",
        "    # sequence of examples in the PyTorch UDPOS dataset. If you\n",
        "    # suspect the sequence is different for your dataset, please\n",
        "    # adapt the test.\n",
        "\n",
        "    train_iter_0 = datasets.UDPOS(split = 'train')    \n",
        "    train_iter_demo = preprocess_data_seq(train_iter_0, 1)\n",
        "    ex0 = (('<s>', 'Al', '-'), \n",
        "           ('<STAG>', 'PROPN', 'PUNCT'), \n",
        "           ('<STAG>', 'NNP', 'HYPH'))\n",
        "    ex1 = (('Al', '-', 'Zaman'), \n",
        "           ('PROPN', 'PUNCT', 'PROPN'), \n",
        "           ('NNP', 'HYPH', 'NNP'))\n",
        "    ex2 = (('-', 'Zaman', ':'), \n",
        "           ('PUNCT', 'PROPN', 'PUNCT'), \n",
        "           ('HYPH', 'NNP', ':'))\n",
        "    assert ex0 == next(train_iter_demo)\n",
        "    assert ex1 == next(train_iter_demo)\n",
        "    assert ex2 == next(train_iter_demo)\n",
        "    print('Test passed.')\n",
        "    \n",
        "test_preprocess_data_seq()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qTo3Bx1o-tB"
      },
      "source": [
        "### Create vocabularies for words and tags\n",
        "\n",
        "Since the number of tags is reasonably large, we use the `Vocab` class to create objects `vocab_words`, `vocab_ud`, and `vocab_ptb` for both words and the two sets of tags from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg5VeN-4o-tC"
      },
      "outputs": [],
      "source": [
        "# It is important to recreate the training set afresh \n",
        "# each time they are used, since they are Python iterators, and\n",
        "# once use, cannot be reused.  One could, however, cast them into\n",
        "# a list, which would store them permanently.\n",
        "\n",
        "train_iter_0 = datasets.UDPOS(split='train')    \n",
        "train_iter_vocab = preprocess_data_seq(train_iter_0, 1)\n",
        "\n",
        "counter_words = Counter()\n",
        "counter_ud = Counter()\n",
        "counter_ptb = Counter()\n",
        "\n",
        "for (text, pos_ud, pos_ptb) in train_iter_vocab:\n",
        "    counter_words.update(text)\n",
        "    counter_ud.update(pos_ud)\n",
        "    counter_ptb.update(pos_ptb)\n",
        "\n",
        "vocab_words = vocab(counter_words)    \n",
        "vocab_ud = vocab(counter_ud)\n",
        "vocab_ptb = vocab(counter_ptb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1rNWClEo-tC"
      },
      "source": [
        "### Collate function for customized data loader\n",
        "\n",
        "**Task 2** [5]: Write a collate function that takes a batch of examples, and for a given window width and tag type, returns the following\n",
        "- a tensor corresponding to the index of the tag for the center word in each example of the batch. Recall, each example is a window with a width of $(2w + 1)$ words.\n",
        "- a tensor corresponding to the index of each of the words in the example, according the to vocabulary for words.\n",
        "\n",
        "Note: The collate function is called by the data loader with the batch of examples from the data set.  So any additional parameters to the function should have default values.\n",
        "\n",
        "From this task onwards we'll only work with the UD tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ncgn90KXo-tD",
        "outputId": "d576ca3a-b356-4b59-8cae-7fb90eafc9e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "TAG = 'ud'\n",
        "\n",
        "def collate_fn(batch, w = W, tag = TAG):\n",
        "    ## WRITE YOUR CODE BELOW\n",
        "    vocab_words_itos = vocab_words.get_itos()\n",
        "    vocab_ud_itos = vocab_ud.get_itos()\n",
        "\n",
        "    labels = torch.zeros(len(batch), dtype=torch.int64).to(device)\n",
        "    word_idxs = torch.zeros(len(batch), len(batch[0][0]), dtype=torch.int64).to(device)\n",
        "    for i in range(len(batch)):\n",
        "        tag = batch[i][1][2]\n",
        "        if tag in vocab_ud_itos:\n",
        "            labels[i] = vocab_ud_itos.index(tag)\n",
        "        for j in range(len(batch[i][0])):\n",
        "            word = batch[i][0][j]\n",
        "            if word in vocab_words_itos:\n",
        "                word_idxs[i,j] = vocab_words_itos.index(word)\n",
        "    return labels, word_idxs\n",
        "        \n",
        "    ## WRITE YOUR CODE ABOVE\n",
        "    # The tensors you return should be place in the correct device\n",
        "    # as shown below.\n",
        "    return labels.to(device), word_idxs.cuda()\n",
        "\n",
        "def test_collate():\n",
        "    pos = [5, 6, 1, 4]  \n",
        "    examples = []\n",
        "    vocab_words_itos = vocab_words.get_itos()\n",
        "    vocab_ud_itos = vocab_ud.get_itos()\n",
        "    vocab_ptb_itos = vocab_ptb.get_itos()\n",
        "\n",
        "    for perm in ['03022', '33210', '33211', '11101']:\n",
        "        words = []\n",
        "        utags = []\n",
        "        ptags = []\n",
        "        for ind in perm:\n",
        "            ind = int(ind)\n",
        "            words.append(vocab_words_itos[pos[ind]])\n",
        "            utags.append(vocab_ud_itos[pos[ind]+1])\n",
        "            ptags.append(vocab_ptb_itos[pos[ind]])\n",
        "        examples.append((words, utags, ptags))\n",
        "    lt =  torch.tensor([6, 2, 2, 7]).to(device)\n",
        "    wt = torch.tensor([\n",
        "        [5, 4, 5, 1, 1],\n",
        "        [4, 4, 1, 6, 5],\n",
        "        [4, 4, 1, 6, 6],\n",
        "        [6, 6, 6, 5, 6]]).to(device)\n",
        "    rlt, rwt = collate_fn(examples, w = 2)\n",
        "    assert torch.equal(lt, rlt)\n",
        "    assert torch.equal(wt, rwt)\n",
        "    print(\"Test passed\")\n",
        "\n",
        "test_collate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77r-oPoVo-tD"
      },
      "source": [
        "### Writing the class for the neural network model\n",
        "\n",
        "**Task 3** [5]: Write a class that implements the following neural network for input $(w_1, w_2, w_3)$ and label $y$ corresponding to tag of $w_2$. (Also discussed in class on April 15, 2021).\n",
        "$$\n",
        "\\begin{eqnarray*}\n",
        "x &=& [E_{[w_1]}, E_{[w_2]}, E_{[w_3]}] \\\\\n",
        "h &=& \\text{tanh}(x W^1 + b^1)\\\\\n",
        "\\tilde{y} & = & hW^2 + b^2\\\\\n",
        "\\hat{y} & = & \\text{softmax}(y)\\\\\n",
        "\\end{eqnarray*}\n",
        "$$\n",
        "The model returns $\\ln \\hat{y}$, and expects the training loop to use `nn.NLLLoss()` to compute the cross entropy loss, given $y$. Assume the embedding has vectors of size $300$, and $h$ is a vector of size $128$.  Please read the documentation for `nn.Embedding(),` `nn.Linear(),`, and `nn.Tanh()` to learn how to use them.\n",
        "\n",
        "*Hint:* Unlike HW2, here the input (for each example in a batch) consist of $(2w+1)$ numbers corresponding to the indices of the words in the window. These are converted to $(2w+1)$ vectors when they are passed to the embedding layer. They have to be concatenated into a single vector of length $(2w+1) \\times 300$ before they are sent to the first linear layer ($W^1$).  Use `reshape()` or `view()` in the `forward()` function below to achieve this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5WZsBlFo-tD"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class NNPOSTagger(nn.Module):\n",
        "    def __init__(self,\n",
        "                 window_size,\n",
        "                 vocab_size, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 nonlinearity, \n",
        "                 # These are used for later tasks\n",
        "                 use_glove = False, \n",
        "                 freeze_glove = False):      \n",
        "        super(NNPOSTagger, self).__init__()\n",
        "        \n",
        "        ## WRITE YOUR CODE BELOW \n",
        "        self.window_size = window_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.nonlinearity = nonlinearity\n",
        "        self.Embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.Linear1 = nn.Linear(window_size * embedding_dim, hidden_dim)\n",
        "        self.Linear2 = nn.Linear(hidden_dim, output_dim)\n",
        "      \n",
        "        \n",
        "    def forward(self, word_idxs_batch):\n",
        "        \n",
        "        ## WRITE YOUR CODE BELOW.\n",
        "        embeds = self.Embedding(word_idxs_batch).reshape(len(word_idxs_batch), self.window_size * self.embedding_dim)\n",
        "        out = self.nonlinearity(self.Linear1(embeds))\n",
        "        out = self.Linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5xAZ4C2o-tE"
      },
      "source": [
        "#### Instantiating the model\n",
        "\n",
        "PyTorch uses `to(device)` to specify whether the model is to be used with a CPU or with a GPU/CUDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw9n6Tj9o-tE"
      },
      "outputs": [],
      "source": [
        "model = NNPOSTagger(window_size = WINDOW_SIZE, \n",
        "                    vocab_size = len(vocab_words), \n",
        "                     embedding_dim = 300, \n",
        "                     hidden_dim = 128, \n",
        "                     output_dim = len(vocab_ud),\n",
        "                     nonlinearity = nn.Tanh(), \n",
        "                     use_glove = False,\n",
        "                     freeze_glove = False).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvD3oTSko-tF"
      },
      "source": [
        "### Training an epoch\n",
        "\n",
        "**Task 4** [5]: Write a function to train one epoch of the dataset. This is similar to the code for the logistic regression based text classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrt2usm0o-tF"
      },
      "outputs": [],
      "source": [
        "loss_function = torch.nn.NLLLoss()\n",
        "\n",
        "def train_an_epoch(dataloader):\n",
        "    ## WRITE YOUR CODE BELOW\n",
        "    model.train() # Sets the module in training mode.\n",
        "    log_interval = 500\n",
        "\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        model.zero_grad()\n",
        "        log_probs = model(text)\n",
        "        loss = loss_function(log_probs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2rAsJmUo-tF"
      },
      "source": [
        "### Computing the accuracy\n",
        "\n",
        "The function is given below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPpqE1Vlo-tF"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(dataloader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():    \n",
        "        total_acc, total_count = 0, 0\n",
        "        for idx, (label, word_idxs) in enumerate(dataloader):\n",
        "            log_probs = model(word_idxs)\n",
        "            total_acc += (log_probs.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTr77xMKo-tG"
      },
      "source": [
        "### Creating the data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GsSzbNHo-tG"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64 # batch size for training\n",
        "  \n",
        "train_0, valid_0, test_0 = train_data_0 = datasets.UDPOS(\n",
        "    split = ('train', 'valid', 'test'))\n",
        "train_data = list(preprocess_data_seq(train_0, W))\n",
        "valid_data = list(preprocess_data_seq(valid_0, W))\n",
        "test_data = list(preprocess_data_seq(test_0, W))\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, \n",
        "                              collate_fn=collate_fn)\n",
        "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, \n",
        "                              collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False, \n",
        "                             collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjoZFjZvo-tG"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVBZjQkPo-tG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "outputId": "fa87ccee-bad7-4fd9-a728-d8f7de4ba48e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time taken: 86.4s, validation accuracy: 0.765.\n",
            "Epoch: 2, time taken: 85.2s, validation accuracy: 0.799.\n",
            "Epoch: 3, time taken: 85.4s, validation accuracy: 0.816.\n",
            "Epoch: 4, time taken: 85.2s, validation accuracy: 0.829.\n",
            "Epoch: 5, time taken: 85.2s, validation accuracy: 0.839.\n",
            "Epoch: 6, time taken: 86.5s, validation accuracy: 0.845.\n",
            "Epoch: 7, time taken: 85.1s, validation accuracy: 0.852.\n",
            "Epoch: 8, time taken: 85.2s, validation accuracy: 0.857.\n",
            "Epoch: 9, time taken: 85.3s, validation accuracy: 0.862.\n",
            "Epoch: 10, time taken: 86.1s, validation accuracy: 0.863.\n",
            "Epoch: 11, time taken: 85.1s, validation accuracy: 0.867.\n",
            "Epoch: 12, time taken: 85.1s, validation accuracy: 0.869.\n",
            "Epoch: 13, time taken: 85.2s, validation accuracy: 0.870.\n",
            "Epoch: 14, time taken: 85.3s, validation accuracy: 0.870.\n",
            "Epoch: 15, time taken: 86.4s, validation accuracy: 0.874.\n",
            "Epoch: 16, time taken: 85.5s, validation accuracy: 0.874.\n",
            "Epoch: 17, time taken: 85.5s, validation accuracy: 0.875.\n",
            "Epoch: 18, time taken: 85.3s, validation accuracy: 0.876.\n",
            "Epoch: 19, time taken: 86.4s, validation accuracy: 0.875.\n",
            "Epoch: 20, time taken: 85.6s, validation accuracy: 0.876.\n",
            "Epoch: 21, time taken: 85.3s, validation accuracy: 0.876.\n",
            "Epoch: 22, time taken: 85.4s, validation accuracy: 0.876.\n",
            "Epoch: 23, time taken: 86.5s, validation accuracy: 0.877.\n",
            "Epoch: 24, time taken: 85.6s, validation accuracy: 0.878.\n",
            "Epoch: 25, time taken: 85.4s, validation accuracy: 0.878.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f296ffd3a90>]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "EPOCHS = 3 # epoch\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "accuracies=[]\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train_an_epoch(train_dataloader)\n",
        "    accuracy = get_accuracy(valid_dataloader)\n",
        "    accuracies.append(accuracy)\n",
        "    time_taken = time.time() - epoch_start_time\n",
        "    print(f'Epoch: {epoch}, time taken: {time_taken:.1f}s, validation accuracy: {accuracy:.3f}.')\n",
        "    \n",
        "plt.plot(range(1, EPOCHS+1), accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmP7UC7Po-tH"
      },
      "source": [
        "### Running on Google Colab\n",
        "\n",
        "Google Colab provides GPUs on which we can run Jupyter notebooks. These are for free as long as the job takes less than, say, 4 hours. Our job will take much less.\n",
        "\n",
        "Please create an account on Google if you don't have one.  Please read a tutorial on using Colab (such as [this](https://colab.research.google.com/notebooks/intro.ipynb)).  Upload your notebook on Colab. **Change the runtime type to GPU.** Make any changes required to get your code to run on Colab. (Normally there should  be no change required.) \n",
        "\n",
        "For all following tasks that required running your code for several epochs, we recommend developing your code locally, but finally running it on Colab to get the results.\n",
        "\n",
        "**Task 5** [5]: Run the model for a sufficient number of epochs (not more than 30) such that the model shows overfitting, if at all, and submit a pdf of the plot of accuracy against number of epochs. Determine the optimal number of epochs to train for. Write code to estimate the accuracy of your model (using the test set) corresponding to this optimal number of epocs and report this estimated accuracy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "EPOCHS = 15 # epoch\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_an_epoch(train_dataloader)\n",
        "\n",
        "accuracy = get_accuracy(test_dataloader)"
      ],
      "metadata": {
        "id": "U4YC5ju3qu2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zMcXZVm0oHj",
        "outputId": "644a68d6-c916-4c70-8f37-392d8103bbee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8584292943379687"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELDqQBEKo-tH"
      },
      "source": [
        "### Initializing with pre-trained embeddings\n",
        "\n",
        "In the above model, the embedding matrix is initialized to either zero or random vectors, but trained along with the rest of the parameters. It may help to initialize the embedding matrix from GloVe vector embeddings. You may then choose to freeze the embedding matrix (i.e., not update the vectors during training) or train them to adapt to the POS examples. (For further details you may optionally read Section 10.1--3 from the Goldberg textbook.)\n",
        "\n",
        "The following code snippet creates a embedding matrix that has a vector corresponding to each of our `vocab_words` created from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NouT6jiZo-tH"
      },
      "outputs": [],
      "source": [
        "from torchtext import vocab\n",
        "\n",
        "glove = vocab.GloVe('6B',cache=VECTORS_CACHE_DIR)\n",
        "glove_vectors = glove.get_vecs_by_tokens(vocab_words.get_itos())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "etYTRiP7AMr5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONM230q7o-tH"
      },
      "source": [
        "**Task 6** [5]: Rewrite/extend the code for the module to initialize the embedding layer with `glove_vectors` created above.  Please read the documentation for the function [`nn.Embedding.from_pretrained()`](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html#torch.nn.Embedding.from_pretrained) to understand how this is done. Submit corresponding plot and report on the performance when the embedding is frozen.\n",
        "\n",
        "**Task 7** [5]: Submit corresponding plot and report on the performance when the embedding is not frozen."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class NNPOSTagger(nn.Module):\n",
        "    def __init__(self,\n",
        "                 window_size,\n",
        "                 vocab_size, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 nonlinearity, \n",
        "                 # These are used for later tasks\n",
        "                 use_glove = True, \n",
        "                 freeze_glove = False):      \n",
        "        super(NNPOSTagger, self).__init__()\n",
        "        \n",
        "        ## WRITE YOUR CODE BELOW \n",
        "        self.window_size = window_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.nonlinearity = nonlinearity\n",
        "        \n",
        "        if use_glove:\n",
        "            self.Embedding = nn.Embedding.from_pretrained(glove_vectors, freeze=freeze_glove)\n",
        "        else:\n",
        "            self.Embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.Linear1 = nn.Linear(window_size * embedding_dim, hidden_dim)\n",
        "        self.Linear2 = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, word_idxs_batch):\n",
        "        \n",
        "        ## WRITE YOUR CODE BELOW.\n",
        "        embeds = self.Embedding(word_idxs_batch).reshape(len(word_idxs_batch), self.window_size * self.embedding_dim)\n",
        "        out = self.nonlinearity(self.Linear1(embeds))\n",
        "        out = self.Linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "zwUGHJRPAZ1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NNPOSTagger(window_size = WINDOW_SIZE, \n",
        "                    vocab_size = len(vocab_words), \n",
        "                     embedding_dim = 300, \n",
        "                     hidden_dim = 128, \n",
        "                     output_dim = len(vocab_ud),\n",
        "                     nonlinearity = nn.Tanh(), \n",
        "                     use_glove = True,\n",
        "                     freeze_glove = True).to(device)"
      ],
      "metadata": {
        "id": "vOSYYdsgAc-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "EPOCHS = 15 # epoch\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "accuracies=[]\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train_an_epoch(train_dataloader)\n",
        "    accuracy = get_accuracy(valid_dataloader)\n",
        "    accuracies.append(accuracy)\n",
        "    time_taken = time.time() - epoch_start_time\n",
        "    print(f'Epoch: {epoch}, time taken: {time_taken:.1f}s, validation accuracy: {accuracy:.3f}.')\n",
        "    \n",
        "plt.plot(range(1, EPOCHS+1), accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "vKpqlR2hAoDg",
        "outputId": "701e760e-0fbf-4c75-f4ce-2e3f3667e9cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time taken: 79.0s, validation accuracy: 0.761.\n",
            "Epoch: 2, time taken: 78.7s, validation accuracy: 0.789.\n",
            "Epoch: 3, time taken: 79.0s, validation accuracy: 0.799.\n",
            "Epoch: 4, time taken: 79.1s, validation accuracy: 0.807.\n",
            "Epoch: 5, time taken: 78.9s, validation accuracy: 0.813.\n",
            "Epoch: 6, time taken: 79.2s, validation accuracy: 0.818.\n",
            "Epoch: 7, time taken: 78.4s, validation accuracy: 0.822.\n",
            "Epoch: 8, time taken: 78.4s, validation accuracy: 0.829.\n",
            "Epoch: 9, time taken: 78.6s, validation accuracy: 0.830.\n",
            "Epoch: 10, time taken: 77.6s, validation accuracy: 0.835.\n",
            "Epoch: 11, time taken: 78.1s, validation accuracy: 0.837.\n",
            "Epoch: 12, time taken: 78.1s, validation accuracy: 0.841.\n",
            "Epoch: 13, time taken: 79.1s, validation accuracy: 0.844.\n",
            "Epoch: 14, time taken: 77.9s, validation accuracy: 0.844.\n",
            "Epoch: 15, time taken: 78.2s, validation accuracy: 0.845.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8b79807d90>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_accuracy(test_dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfmgLY6BAr-x",
        "outputId": "abdf9696-4356-49a2-c4ab-4afe74454f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8469936645814241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NNPOSTagger(window_size = WINDOW_SIZE, \n",
        "                    vocab_size = len(vocab_words), \n",
        "                     embedding_dim = 300, \n",
        "                     hidden_dim = 128, \n",
        "                     output_dim = len(vocab_ud),\n",
        "                     nonlinearity = nn.Tanh(), \n",
        "                     use_glove = True,\n",
        "                     freeze_glove = False).to(device)"
      ],
      "metadata": {
        "id": "B-qF3uRdLENv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "EPOCHS = 15 # epoch\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "accuracies=[]\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train_an_epoch(train_dataloader)\n",
        "    accuracy = get_accuracy(valid_dataloader)\n",
        "    accuracies.append(accuracy)\n",
        "    time_taken = time.time() - epoch_start_time\n",
        "    print(f'Epoch: {epoch}, time taken: {time_taken:.1f}s, validation accuracy: {accuracy:.3f}.')\n",
        "    \n",
        "plt.plot(range(1, EPOCHS+1), accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "SvTZDg3BLKxY",
        "outputId": "d918449d-3e31-46db-ee48-56c48c852a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time taken: 82.6s, validation accuracy: 0.842.\n",
            "Epoch: 2, time taken: 81.2s, validation accuracy: 0.871.\n",
            "Epoch: 3, time taken: 81.7s, validation accuracy: 0.881.\n",
            "Epoch: 4, time taken: 80.9s, validation accuracy: 0.886.\n",
            "Epoch: 5, time taken: 81.8s, validation accuracy: 0.889.\n",
            "Epoch: 6, time taken: 82.3s, validation accuracy: 0.890.\n",
            "Epoch: 7, time taken: 82.2s, validation accuracy: 0.892.\n",
            "Epoch: 8, time taken: 83.9s, validation accuracy: 0.891.\n",
            "Epoch: 9, time taken: 82.2s, validation accuracy: 0.892.\n",
            "Epoch: 10, time taken: 82.9s, validation accuracy: 0.892.\n",
            "Epoch: 11, time taken: 82.6s, validation accuracy: 0.893.\n",
            "Epoch: 12, time taken: 82.9s, validation accuracy: 0.892.\n",
            "Epoch: 13, time taken: 84.3s, validation accuracy: 0.891.\n",
            "Epoch: 14, time taken: 83.0s, validation accuracy: 0.891.\n",
            "Epoch: 15, time taken: 82.8s, validation accuracy: 0.891.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8b6eb9d910>]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_accuracy(test_dataloader))"
      ],
      "metadata": {
        "id": "P7UZFBp7LLys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfreezing the embeddings improves performance by about 5%"
      ],
      "metadata": {
        "id": "f8w_eLu5JT1T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyUllvK4o-tH"
      },
      "source": [
        "### Optionally extending the model\n",
        "\n",
        "**Task 8** [0]: This is an optional, open-ended task.   Try three different ways of your choosing to improve the performance of the model. You may want to vary w, or add additional layers to the network, or increase the size of the hidden vectors $h$, or try with different activation functions. Report on the results you get. Do you think your POS tagger is comparable to a human tagger?\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}